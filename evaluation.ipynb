{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gold_path = '../data/test_gd/test.gold.en-de.json'\n",
    "prediction_path = './final_results/en-de_0-shot.json'\n",
    "test_gold_df = pd.read_json(test_gold_path)\n",
    "prediction_df = pd.read_json(prediction_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing acc directly by comparing string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "type_acc = [0.0, 0.0, 0.0, 0.0]\n",
    "type_len = [0.0, 0.0, 0.0, 0.0]\n",
    "type_name = ['prefix', 'suffix', 'bi_context', 'zero_context']\n",
    "for i in range(len(test_gold_df)):\n",
    "    if test_gold_df['context_type'][i] == 'prefix':\n",
    "        if test_gold_df['target'][i].lower() == prediction_df['prediction'][i].lower():\n",
    "            type_acc[0] += 1.0\n",
    "        type_len[0] += 1.0\n",
    "    elif test_gold_df['context_type'][i] == 'suffix':\n",
    "        if test_gold_df['target'][i].lower() == prediction_df['prediction'][i].lower():\n",
    "            type_acc[1] += 1.0\n",
    "        type_len[1] += 1.0\n",
    "    elif test_gold_df['context_type'][i] == 'bi_context':\n",
    "        if test_gold_df['target'][i].lower() == prediction_df['prediction'][i].lower():\n",
    "            type_acc[2] += 1.0\n",
    "        type_len[2] += 1.0\n",
    "    else:\n",
    "        if test_gold_df['target'][i].lower() == prediction_df['prediction'][i].lower():\n",
    "            type_acc[3] += 1.0\n",
    "        type_len[3] += 1.0\n",
    "    if test_gold_df['target'][i].lower() == prediction_df['prediction'][i].lower():\n",
    "        acc += 1.0\n",
    "print(\"The acc is {}\".format(acc/len(test_gold_df)))\n",
    "for i in range(len(type_acc)):\n",
    "    print(\"The acc of {} is {}\".format(type_name[i], type_acc[i]/type_len[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute acc with lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "#nlp = spacy.load(\"de_dep_news_trf\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.0\n",
    "for i in trange(len(test_gold_df)):\n",
    "    if prediction_df['prediction'][i] == '':\n",
    "        continue\n",
    "    temp1 = [token.lemma_ for token in nlp(test_gold_df['target'][i].lower())][0] \n",
    "    temp2 = [token.lemma_ for token in nlp(prediction_df['prediction'][i].lower())][0]\n",
    "    if temp1 == temp2:\n",
    "        acc += 1.0\n",
    "print(acc/len(test_gold_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the amount of predictions that don't start with the typed sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault1_rate = 0.0\n",
    "for i in trange(len(test_gold_df)):\n",
    "    if not prediction_df['prediction'][i].lower().startswith(prediction_df['typed_seq'][i].lower()):\n",
    "        fault1_rate+=1.0\n",
    "print(\"The error rate of predictions don't start with the typed sequence is {}\".format(fault1_rate/len(test_gold_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the amount of generation that contains a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_type = 'de-en'\n",
    "shot = 0\n",
    "raw_prediction_df = pd.read_json('./results/inference_{}_{}-shot.json'.format(language_type, shot))\n",
    "fault2_rate = 0.0\n",
    "for i in range(len(raw_prediction_df)):\n",
    "    generation = raw_prediction_df['generation'][i]\n",
    "    str_list = generation.split('Answer: ')\n",
    "    if len(str_list)>1:\n",
    "        str_list = str_list[1].split(' ')\n",
    "    else:\n",
    "        str_list = str_list[0].split(' ')\n",
    "    if len(str_list)>1:\n",
    "        fault2_rate+=1.0\n",
    "print(fault2_rate/len(raw_prediction_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wlac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
